---
layout: post
title:  "GNN"
date:   2021-01-06 21:03:36 +0530
categories: DeepLearning GraphNeuralNetwork
---

Graph neural networks (GNNs) are connectionist models that capture the dependence of graphs via message passing between the nodes of graphs. Unlike standard neural networks, graph neural networks retain a state that can represent information from its neighborhood with arbitrary depth. Although the primitive GNNs have been found difficult to train for a fixed point, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful learning with them.

# Introduction
Graphs are a kind of data structure which models a set
of objects (nodes) and their relationships (edges). As a unique **non-Euclidean** data structure for machine learning, graph analysis focuses on **node classification, link prediction, and clustering**.  
Graph neural networks (GNNs) are deep learning based methods that operate on graph domain.  
GNN Motivation roots in CNN. 
### CNN
- able to extract multi-scale localized spatial features and compose them to construct highly expressive representations.
- key: local connection, shared weights and the use of multi-layer    

GNN has the following edges:  
- GNNs propagate on each node respecively (no specific order)
- GNNs can do propagation guided by the graph structure instead of using dependency information as part of features, able to uodate the hidden state of nodes
- perform reasoning

# Graph Neural Networks
First proposed in [1].
The target of GNN is to learn a state embedding $h_v \in R^{s}$ which contains the information of neighborhood for each node. The state embedding hv is an s-dimension vector of node v and can be used to produce an output $o_v$ such as the node label.  
$$h_v = f(x_v,x_{co[v]},h_{ne[v]},x_{ne[v]})$$
$$o_v = g(h_v,x_v)$$  
$$H=F(H,X)$$  
$$O=G(H,X_N)$$  
$$H^{t+1}=F(H^t,X)$$  
t denotes the t-th iteration  
$$loss=\sum^p_{i=1}(t_i-o_i)$$  
> - The states $h_v^t$ are iteratively updated by Eq. 1 until a time T. They approach the fixed point solution of Eq. 3: H(T) ≈ H. 
> - The gradient of weights W is computed from the loss.
> - The weights W are updated according to the **gradient-descent strategy** computed in the last step.

### Limitations
- inefficient to update the hidden states of nodes iteratively for the fixed point
-  hierarchical feature extraction method
-  unable to learn the hidden states of edges
- unsuitable to use the fixed points if we focus on the **representation of nodes** instead of graphs
<img src="https://alwaysbyx.github.io/assets/GNN_propagation.png" style="zoom:80%" /> 

## Graph Convolutional Network
- define Fourier Transformation in Graph when investigating GSP(graph signal processing)
- define convolution in graph
- aggregated with deep learning to propose GCN  
<img src="https://alwaysbyx.github.io/assets/gcn-01.png"/> 
常用的拉普拉斯矩阵有三种：
1. $L=D-A$ Combinatorial Laplacian
2. $L^{sys}=D^{-\frac{1}{2}}LD^{-\frac{1}{2}}$ Symmetric normalized Laplacian (often used)
3. $L^{rw}=D^{-1}L$ random walk normalized Laplacian  
$$y_{output} = \sigma(\sum_{j=0}^{K-1}\alpha_jL^jx)$$
- 卷积核只具有k个参数
- convolutional filter具有很好的spatial localization,k代表receptive field
- L不需要进行特征分解，

# Reference
[1]F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini, “The graph neural network model,” IEEE TNN 2009, vol. 20,
no. 1, pp. 61–80, 2009.  
[2]Graph Neural Networks: A Review of Methods and Applications

