---
layout: post
title:  "GNN"
date:   2021-01-06 21:03:36 +0530
categories: DeepLearning GraphNeuralNetwork
---

$$H^{t+1} = \hat{A}H^{t}W$$
$$\hat{A} = I_n + D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$$

# Introduction
Graphs are a kind of data structure which models a set
of objects (nodes) and their relationships (edges). As a unique **non-Euclidean** data structure for machine learning, graph analysis focuses on **node classification, link prediction, and clustering**.  
Graph neural networks (GNNs) are deep learning based methods that operate on graph domain.  
GNN Motivation roots in CNN. 
### CNN
- able to extract multi-scale localized spatial features and compose them to construct highly expressive representations.
- key: local connection, shared weights and the use of multi-layer    

GNN has the following edges:  
- GNNs propagate on each node respecively (no specific order)
- GNNs can do propagation guided by the graph structure instead of using dependency information as part of features, able to uodate the hidden state of nodes
- perform reasoning

# Graph Neural Networks
First proposed in [1].
The target of GNN is to learn a state embedding $h_v \in R^{s}$ which contains the information of neighborhood for each node. The state embedding hv is an s-dimension vector of node v and can be used to produce an output $o_v$ such as the node label.  
$$h_v = f(x_v,x_{co[v]},h_{ne[v]},x_{ne[v]})$$
$$o_v = g(h_v,x_v)$$
$$H=F(H,X)$$
$$O=G(H,X_N)$$
$$H^{t+1}=F(H^t,X)$$
t denotes the t-th iteration
$$loss=\sum^p_{i=1}(t_i-o_i)$$
> - The states $h_v^t$ are iteratively updated by Eq. 1 until a time T. They approach the fixed point solution of Eq. 3: H(T) ≈ H. 
> - The gradient of weights W is computed from the loss.
> - The weights W are updated according to the **gradient-descent strategy** computed in the last step.
### Limitations
- inefficient to update the hidden states of nodes iteratively for the fixed point
-  hierarchical feature extraction method
-  unable to learn the hidden states of edges
- unsuitable to use the fixed points if we focus on the **representation of nodes** instead of graphs
<img src="https://alwaysbyx.github.io/assets/GNN_propagation.png" style="zoom:60%" /> 

# Reference
[1]F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini, “The graph neural network model,” IEEE TNN 2009, vol. 20,
no. 1, pp. 61–80, 2009.  
[2]Graph Neural Networks: A Review of Methods and Applications

